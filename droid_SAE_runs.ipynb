{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9040451",
   "metadata": {},
   "source": [
    "# Droid PaliGemma SAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e229ce0",
   "metadata": {},
   "source": [
    "### Import libraries and define SAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d582a-23d9-40ac-8dbb-e86e940d203a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# this is to only use GPUs 0 and 1\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torchvision.utils import make_grid\n",
    "import requests\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import multiprocessing as mp\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_in, d_hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.decoder = nn.Linear(d_hidden, d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.encoder(x))\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c8b484-d839-4868-b8ed-1313f8e59320",
   "metadata": {},
   "source": [
    "### Load Droid Dataset\n",
    "\n",
    "There are three options of datasets to collect activations then run SAE on. \n",
    "\n",
    "1) All episodes, 20 frames each, equally spaced out: \"100_episodes_20_frames_each.pt\"\n",
    "2) 10 episodes, all frames \"10_episodes_all_frames.pt\"\n",
    "3) A collage of 6 equally spaced frames in one image--100 images total. \"droid_collages_896.pt\"\n",
    "\n",
    "Choose the path that you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32712b-75ec-4de9-ae94-8d346a9525a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"droid_datasets\"  # root directory for datasets\n",
    "\n",
    "available_datasets = {1: \"100_episodes_4_frames_each.pt\", 2: \"10_episodes_all_frames.pt\", 3: \"droid_collages_896.pt\"}\n",
    "\n",
    "# look at above cell for options\n",
    "chosen_dataset = 1\n",
    "\n",
    "dataset_path = os.path.join(DATASET_DIR, available_datasets[chosen_dataset])\n",
    "print(f\"Using dataset: {dataset_path}\")\n",
    "CAPTION_PROMPT = \"describe the task the robot is taking:\"\n",
    "\n",
    "class DroidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        data = torch.load(path)\n",
    "        self.images = data[\"images\"]\n",
    "        # self.prompts = data[\"prompts\"]\n",
    "        # Describe image prompt \n",
    "        self.prompts = [CAPTION_PROMPT for _ in self.images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"image\": self.images[idx],\n",
    "            \"prompt\": self.prompts[idx]\n",
    "        }\n",
    "dataset = DroidDataset(path=dataset_path)\n",
    "print(f\"Loaded dataset: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f8bac",
   "metadata": {},
   "source": [
    "### Load VLM model\n",
    "\n",
    "There are three options VLAs to choose from:\n",
    "\n",
    "1) Base Paligemma Model: \"base\"\n",
    "2) Finetuned with single images to generate the task instruction: \"finetuned_single\"\n",
    "3) Finetuned with collated images to generate the task instruction: \"finetuned_collage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cbc5e-a5d0-425c-8c2f-043631542dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BIG_MODEL_ID = \"google/paligemma2-3b-pt-896\"\n",
    "SMALL_MODEL_ID = \"google/paligemma2-3b-pt-224\"\n",
    "FINETUNE_PATH_SINGLE_896 = \"../finetuned_paligemma_single.pt\"\n",
    "FINETUNE_PATH_SINGLE_224 = \"../paligemma_converted_jax_weights_multimodal.pt\"\n",
    "FINETUNE_PATH_COLLAGE = \"../finetuned_paligemma.pt\"\n",
    "\n",
    "available_models = {\n",
    "    \"big_base\": {\"model_id\": BIG_MODEL_ID, \"finetuned\": False},\n",
    "    \"small_base\": {\"model_id\": SMALL_MODEL_ID, \"finetuned\": False},\n",
    "    \"finetuned_single_896\": {\"model_id\": BIG_MODEL_ID, \"finetuned\": True, \"path\": FINETUNE_PATH_SINGLE_896},\n",
    "    \"finetuned_single_224\": {\"model_id\": SMALL_MODEL_ID, \"finetuned\": True, \"path\": FINETUNE_PATH_SINGLE_224},\n",
    "    \"finetuned_collage\": {\"model_id\": BIG_MODEL_ID, \"finetuned\": True, \"path\": FINETUNE_PATH_COLLAGE},\n",
    "}\n",
    "\n",
    "chosen_model = \"finetuned_single_224\"  # options: big_base, small_base, finetuned_single_896, finetuned_single_224, finetuned_collage\n",
    "\n",
    "model_info = available_models[chosen_model]\n",
    "processor = AutoProcessor.from_pretrained(model_info[\"model_id\"])\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    model_info[\"model_id\"],\n",
    "    torch_dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "if model_info.get(\"finetuned\", False):\n",
    "    checkpoint = torch.load(model_info[\"path\"], map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()\n",
    "print(f\"Loaded model: {chosen_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31206f6-6826-4b47-a058-9a5c834fb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     # if isinstance(module, torch.nn.Linear):\n",
    "#     #     print(name, \"â†’\", module)\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e4029-5f0d-44b4-a3f9-eb1a368a82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_DICT = {\n",
    "    \"vision_mlp_fc1\": \"vision_tower.vision_model.encoder.layers.6.mlp.fc1\",\n",
    "    \"vision_attn_out\": \"vision_tower.vision_model.encoder.layers.6.self_attn.out_proj\",\n",
    "    \"fusion_proj\": \"multi_modal_projector.linear\",\n",
    "    \"language_mid_layer\": \"language_model.model.layers.12.mlp\",\n",
    "    \"language_final_layer\": \"language_model.model.layers.25.mlp\",\n",
    "}\n",
    "\n",
    "layer_name = LAYER_DICT[\"language_mid_layer\"] \n",
    "\n",
    "if layer_name == LAYER_DICT[\"language_mid_layer\"] or LAYER_DICT[layer_name == \"language_final_layer\"]:\n",
    "    language_layer = True\n",
    "else:\n",
    "    language_layer = False\n",
    "\n",
    "hook_acts = {}\n",
    "\n",
    "def hook(module, input, output):\n",
    "    hook_acts[\"activation\"] = output.detach().cpu()\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def get_module_by_name(model, path):\n",
    "    return reduce(getattr, path.split(\".\"), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057f00d-a5df-4c3b-b308-fcf95605c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a8841-66c8-4048-967e-b734ee561091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import amp  # for fix 3\n",
    "\n",
    "# === Config ===\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [example[\"image\"] for example in batch]\n",
    "    prompts = [example[\"prompt\"] for example in batch]\n",
    "    return {\"images\": images, \"prompts\": prompts}\n",
    "\n",
    "# === Split dataset across GPUs ===\n",
    "half = len(dataset) // 2\n",
    "datasets = [Subset(dataset, range(0, half)), Subset(dataset, range(half, len(dataset)))]\n",
    "devices = [\"cuda:0\", \"cuda:1\"]\n",
    "activation_lists = [[], []]\n",
    "token_counts = [[], []]\n",
    "captions_lists  = [[], []]\n",
    "\n",
    "from tqdm.auto import tqdm  # use auto for better notebook/thread compatibility\n",
    "from io import StringIO\n",
    "\n",
    "def run_inference(model, dataloader, device, activations_out, token_counts_out, captions_out, layer_path):\n",
    "    model = model.to(device).eval()\n",
    "    hook_acts = {}\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        hook_acts[\"activation\"] = output.detach().cpu()\n",
    "\n",
    "    handle = get_module_by_name(model, layer_path).register_forward_hook(hook)\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        dataloader,\n",
    "        desc=f\"Inference on {device}\",\n",
    "        leave=True,\n",
    "        dynamic_ncols=True,\n",
    "        position=0 if device == \"cuda:0\" else 1  # prevent conflict\n",
    "    )\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        torch.cuda.empty_cache()\n",
    "        hook_acts.clear()\n",
    "\n",
    "        images = batch[\"images\"]\n",
    "        prompts = [f\"<image> {p}\" for p in batch[\"prompts\"]]\n",
    "\n",
    "        model_inputs = processor(\n",
    "            text=prompts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            do_rescale=False\n",
    "        )\n",
    "        model_inputs = {k: v.to(device, non_blocking=True) for k, v in model_inputs.items()}\n",
    "\n",
    "        if language_layer:\n",
    "            with torch.no_grad(), amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                # 1) generate captions\n",
    "                generated_ids = model.generate(\n",
    "                    **model_inputs,\n",
    "                    max_new_tokens=32,       \n",
    "                    num_beams=1,             \n",
    "                    early_stopping=True\n",
    "                )\n",
    "            # 2) decode into strings\n",
    "            batch_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            captions_out.extend(batch_captions)\n",
    "        else:\n",
    "            with torch.no_grad(), amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                _ = model(**model_inputs)\n",
    "\n",
    "        act = hook_acts[\"activation\"]\n",
    "        if isinstance(act, tuple):\n",
    "            act = act[0]\n",
    "\n",
    "        activations_out.append(act.cpu().float())\n",
    "        token_counts_out.extend([act.shape[1]] * act.size(0))\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    \n",
    "from threading import Thread\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "dataloaders = [\n",
    "    DataLoader(datasets[0], batch_size=BATCH_SIZE, collate_fn=collate_fn,\n",
    "               num_workers=NUM_WORKERS, pin_memory=True),\n",
    "    DataLoader(datasets[1], batch_size=BATCH_SIZE, collate_fn=collate_fn,\n",
    "               num_workers=NUM_WORKERS, pin_memory=True)\n",
    "]\n",
    "\n",
    "models = [copy.deepcopy(model), copy.deepcopy(model)]\n",
    "threads = []\n",
    "\n",
    "for i in range(2):\n",
    "    t = Thread(target=run_inference, args=(\n",
    "        models[i], dataloaders[i], devices[i], activation_lists[i], token_counts[i], captions_lists[i], layer_name))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"activation_lists\", [len(activation_lists[i]) for i in range(2)])\n",
    "\n",
    "all_activations = torch.cat(activation_lists[0] + activation_lists[1], dim=0)\n",
    "image_to_token_counts = token_counts[0] + token_counts[1]\n",
    "all_captions = captions_lists[0] + captions_lists[1]\n",
    "\n",
    "if all_activations.size(0) == 0:\n",
    "    raise ValueError(\"No valid activations collected!\")\n",
    "\n",
    "print(\"Collected activations:\", all_activations.shape)\n",
    "print(\"Sum of tokens recorded:\", sum(image_to_token_counts))\n",
    "\n",
    "del models, activation_lists, token_counts, datasets, dataloaders, hook_acts\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd443669",
   "metadata": {},
   "source": [
    "### Train SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_activations.shape\n",
    "print(\"captions: \", len(all_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b6b0e-8735-46fd-9f1e-42033fd83412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config and Hyperparameters ===\n",
    "HIDDEN_MULTIPLIER = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "SPARSITY_WEIGHT = 1e-2\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "NAME_OF_RUN = \"SAE_new_notebook_test\"\n",
    "csv_log_path = \"sae_run_log.csv\"\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Initialize Weights & Biases ===\n",
    "wandb.init(project=\"sparse-autoencoder\", name=NAME_OF_RUN, config={\n",
    "    \"hidden_multiplier\": HIDDEN_MULTIPLIER,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"sparsity_weight\": SPARSITY_WEIGHT,\n",
    "    \"n_epochs\": N_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "})\n",
    "\n",
    "# === Model Setup ===\n",
    "d_in = all_activations.shape[-1]\n",
    "d_hidden = HIDDEN_MULTIPLIER * d_in\n",
    "\n",
    "sae = SparseAutoencoder(d_in=d_in, d_hidden=d_hidden).to(device)\n",
    "sae = torch.nn.DataParallel(sae)  # Split across all visible GPUs\n",
    "\n",
    "optimizer = torch.optim.AdamW(sae.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# === DataLoader ===\n",
    "train_dataset = torch.utils.data.TensorDataset(all_activations)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Training Loop ===\n",
    "epoch_losses = []\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    sae.train()\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"Epoch {epoch+1} started...\")\n",
    "\n",
    "    for (batch,) in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        batch = batch.to(next(sae.parameters()).device, non_blocking=True)\n",
    "        recon, z = sae(batch)\n",
    "\n",
    "        loss = loss_fn(recon, batch) + SPARSITY_WEIGHT * torch.mean(torch.abs(z))\n",
    "        epoch_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataset)\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    wandb.log({\"loss\": avg_loss, \"epoch\": epoch + 1})\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == N_EPOCHS - 1:\n",
    "        print(f\"Epoch {epoch+1}: Loss {avg_loss:.6f}\")\n",
    "\n",
    "\n",
    "# === Save model checkpoint ===\n",
    "model_path = os.path.join(save_dir, f\"sae_{timestamp}.pth\")\n",
    "torch.save({\n",
    "    \"state_dict\": sae.state_dict(),\n",
    "    \"d_in\": d_in,\n",
    "    \"d_hidden\": d_hidden,\n",
    "}, model_path)\n",
    "wandb.save(model_path)\n",
    "print(f\"Saved SAE to {model_path}\")\n",
    "\n",
    "# === Plot and log loss curve ===\n",
    "plt.plot(range(1, N_EPOCHS + 1), epoch_losses, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"SAE Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "wandb.log({\"loss_curve\": wandb.Image(plt)})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "run_config = {\n",
    "    \"run_name\": NAME_OF_RUN,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"hidden_multiplier\": HIDDEN_MULTIPLIER,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"sparsity_weight\": SPARSITY_WEIGHT,\n",
    "    \"n_epochs\": N_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"d_in\": d_in,\n",
    "    \"d_hidden\": d_hidden,\n",
    "    \"final_loss\": avg_loss,\n",
    "    \"model_path\": model_path,\n",
    "    \"checkpoint_name\": os.path.basename(model_path),\n",
    "    \"vlm_model\": chosen_model,\n",
    "    \"hook_layer\": layer_name,\n",
    "    \"dataset_used\": os.path.basename(dataset_path),\n",
    "}\n",
    "\n",
    "write_header = not os.path.exists(csv_log_path)\n",
    "with open(csv_log_path, mode=\"a\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=run_config.keys())\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(run_config)\n",
    "\n",
    "sae = sae.module\n",
    "\n",
    "print(f\"Logged run to {csv_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f511334",
   "metadata": {},
   "source": [
    "### Visualize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9022e-0fa7-486d-a538-6eedd5604fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ==== CONFIG ====\n",
    "top_k = 5\n",
    "n_features_to_plot = 10\n",
    "batch_size = 4096\n",
    "num_workers = 8\n",
    "\n",
    "# Ensure sae.module if wrapped in DataParallel\n",
    "sae_core = sae.module if isinstance(sae, torch.nn.DataParallel) else sae\n",
    "\n",
    "# Flatten activations if shape is [N, T, D]\n",
    "all_activations_flat = all_activations.view(-1, all_activations.shape[-1])\n",
    "\n",
    "# ==== STREAM ENCODE ====\n",
    "sparse_feature_list = []\n",
    "d_hidden = sae_core.encoder.out_features\n",
    "running_sum = torch.zeros(d_hidden, device=device)\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, all_activations_flat.size(0), batch_size):\n",
    "        batch = all_activations_flat[i:i + batch_size].to(device=device, dtype=next(sae.parameters()).dtype)\n",
    "        sparse = sae_core.activation(sae_core.encoder(batch))\n",
    "        running_sum += sparse.sum(dim=0)\n",
    "        count += sparse.size(0)\n",
    "        sparse_feature_list.append(sparse.cpu())\n",
    "\n",
    "sparse_features = torch.cat(sparse_feature_list, dim=0)\n",
    "\n",
    "# ==== TOKEN TO IMAGE INDEX MAPPING ====\n",
    "image_indices_per_token = []\n",
    "for img_idx, n_tokens in enumerate(image_to_token_counts):\n",
    "    image_indices_per_token.extend([img_idx] * n_tokens)\n",
    "image_indices_per_token = np.array(image_indices_per_token)\n",
    "\n",
    "assert sparse_features.shape[0] == len(image_indices_per_token), \\\n",
    "    f\"Mismatch: {sparse_features.shape[0]} activations vs {len(image_indices_per_token)} image-token mappings\"\n",
    "\n",
    "# ==== TOP FEATURES ====\n",
    "mean_features = running_sum / count\n",
    "top_features = torch.topk(mean_features, k=n_features_to_plot).indices.tolist()\n",
    "print(f\"Top {n_features_to_plot} active features:\", top_features)\n",
    "\n",
    "# ==== IMAGE & TEXT LOADER ====\n",
    "def load_img_and_text(idx):\n",
    "    img_idx = int(image_indices_per_token[idx])\n",
    "    entry = dataset[img_idx]\n",
    "    image_tensor = entry[\"image\"]\n",
    "    image_pil = to_pil_image(image_tensor)\n",
    "    if language_layer:\n",
    "        caption = all_captions[img_idx] \n",
    "    else:\n",
    "        caption = entry[\"prompt\"]\n",
    "    return np.array(image_pil), caption\n",
    "\n",
    "# ==== DISPLAY LOOP ====\n",
    "for feature_idx in top_features:\n",
    "    activations = sparse_features[:, feature_idx]\n",
    "    topk_indices = torch.topk(activations, k=top_k).indices.cpu().tolist()\n",
    "\n",
    "    # Load image-prompt pairs\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        img_and_text = list(executor.map(load_img_and_text, topk_indices))\n",
    "\n",
    "    images, captions = zip(*img_and_text)\n",
    "\n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, top_k, figsize=(3 * top_k, 4))\n",
    "    fig.suptitle(f\"Feature {feature_idx} (Top {top_k} Activations)\", fontsize=14)\n",
    "\n",
    "    for ax, img, text in zip(axes, images, captions):\n",
    "        ax.imshow(img)\n",
    "        if language_layer:\n",
    "            ax.set_title(text, fontsize=8, wrap=True)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906de685",
   "metadata": {},
   "source": [
    "### Sparsity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107237bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `sae` is already loaded and on the correct device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sae = sae.to(device)\n",
    "sae.eval()\n",
    "\n",
    "# Compute feature usage (mean |z|) without storing all z\n",
    "d_hidden = sae.module.encoder.out_features if isinstance(sae, torch.nn.DataParallel) else sae.encoder.out_features\n",
    "running_sum = torch.zeros(d_hidden, device=device)\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (batch,) in train_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        _, z = sae(batch)                      # shape: [B, d_hidden] or [B, T, d_hidden]\n",
    "        z = z.view(-1, z.shape[-1])            # flatten in case it's [B, T, d_hidden]\n",
    "        running_sum += torch.sum(torch.abs(z), dim=0)\n",
    "        count += z.shape[0]\n",
    "\n",
    "feature_usage = running_sum / count\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(feature_usage.cpu().numpy(), bins=100)\n",
    "plt.title(\"Feature Usage Distribution\")\n",
    "plt.xlabel(\"Mean |z|\")\n",
    "plt.ylabel(\"# Features\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
