{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9040451",
   "metadata": {},
   "source": [
    "# Droid PaliGemma SAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e229ce0",
   "metadata": {},
   "source": [
    "### Import libraries and define SAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51d582a-23d9-40ac-8dbb-e86e940d203a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 01:55:54.411792: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-05 01:55:54.432422: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746410154.455788 1195932 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746410154.463393 1195932 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746410154.481826 1195932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746410154.481847 1195932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746410154.481850 1195932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746410154.481852 1195932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-05 01:55:54.487604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# this is to only use GPUs 0 and 1\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torchvision.utils import make_grid\n",
    "import requests\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import multiprocessing as mp\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_in, d_hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.decoder = nn.Linear(d_hidden, d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.encoder(x))\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c8b484-d839-4868-b8ed-1313f8e59320",
   "metadata": {},
   "source": [
    "### Load Droid Dataset\n",
    "\n",
    "There are three options of datasets to collect activations then run SAE on. \n",
    "\n",
    "1) All episodes, 20 frames each, equally spaced out: \"100_episodes_20_frames_each.pt\"\n",
    "2) 10 episodes, all frames \"10_episodes_all_frames.pt\"\n",
    "3) A collage of 6 equally spaced frames in one image--100 images total. \"droid_collages_896.pt\"\n",
    "\n",
    "Choose the path that you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd32712b-75ec-4de9-ae94-8d346a9525a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: droid_datasets/100_episodes_4_frames_each.pt\n",
      "Loaded dataset: droid_datasets/100_episodes_4_frames_each.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1195932/2826579454.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"droid_datasets\"  # root directory for datasets\n",
    "\n",
    "available_datasets = {1: \"100_episodes_4_frames_each.pt\", 2: \"10_episodes_all_frames.pt\", 3: \"droid_collages_896.pt\"}\n",
    "\n",
    "# look at above cell for options\n",
    "chosen_dataset = 1\n",
    "\n",
    "dataset_path = os.path.join(DATASET_DIR, available_datasets[chosen_dataset])\n",
    "print(f\"Using dataset: {dataset_path}\")\n",
    "CAPTION_PROMPT = \"describe the task the robot is taking:\"\n",
    "\n",
    "class DroidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        data = torch.load(path)\n",
    "        self.images = data[\"images\"]\n",
    "        # self.prompts = data[\"prompts\"]\n",
    "        # Describe image prompt \n",
    "        self.prompts = [CAPTION_PROMPT for _ in self.images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"image\": self.images[idx],\n",
    "            \"prompt\": self.prompts[idx]\n",
    "        }\n",
    "dataset = DroidDataset(path=dataset_path)\n",
    "print(f\"Loaded dataset: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f8bac",
   "metadata": {},
   "source": [
    "### Load VLM model\n",
    "\n",
    "There are three options VLAs to choose from:\n",
    "\n",
    "1) Base Paligemma Model: \"base\"\n",
    "2) Finetuned with single images to generate the task instruction: \"finetuned_single\"\n",
    "3) Finetuned with collated images to generate the task instruction: \"finetuned_collage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77cbc5e-a5d0-425c-8c2f-043631542dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f99680d07f4238bf1766edaf6bd6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: base\n"
     ]
    }
   ],
   "source": [
    "# MODEL_ID = \"google/paligemma2-3b-pt-896\"\n",
    "MODEL_ID = \"google/paligemma-3b-mix-224\"\n",
    "FINETUNE_PATH_SINGLE = \"/home/henrytsai/dhruv/roboterp/finetuned_paligemma_single.pt\"\n",
    "FINETUNE_PATH_COLLAGE = \"/home/henrytsai/dhruv/roboterp/finetuned_paligemma.pt\"\n",
    "\n",
    "available_models = {\n",
    "    \"base\": {\"model_id\": MODEL_ID, \"finetuned\": False},\n",
    "    \"finetuned_single\": {\"model_id\": MODEL_ID, \"finetuned\": True, \"path\": FINETUNE_PATH_SINGLE},\n",
    "    \"finetuned_collage\": {\"model_id\": MODEL_ID, \"finetuned\": True, \"path\": FINETUNE_PATH_COLLAGE},\n",
    "}\n",
    "\n",
    "chosen_model = \"base\" \n",
    "\n",
    "model_info = available_models[chosen_model]\n",
    "processor = AutoProcessor.from_pretrained(model_info[\"model_id\"])\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    model_info[\"model_id\"],\n",
    "    torch_dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "if model_info.get(\"finetuned\", False):\n",
    "    checkpoint = torch.load(model_info[\"path\"], map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()\n",
    "print(f\"Loaded model: {chosen_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31206f6-6826-4b47-a058-9a5c834fb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     # if isinstance(module, torch.nn.Linear):\n",
    "#     #     print(name, \"→\", module)\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "252e4029-5f0d-44b4-a3f9-eb1a368a82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_DICT = {\n",
    "    \"vision_mlp_fc1\": \"vision_tower.vision_model.encoder.layers.6.mlp.fc1\",\n",
    "    \"vision_attn_out\": \"vision_tower.vision_model.encoder.layers.6.self_attn.out_proj\",\n",
    "    \"fusion_proj\": \"multi_modal_projector.linear\",\n",
    "    \"language_mlp_down\": \"language_model.model.layers.6.mlp.down_proj\",\n",
    "}\n",
    "\n",
    "layer_name = LAYER_DICT[\"fusion_proj\"] \n",
    "\n",
    "hook_acts = {}\n",
    "\n",
    "def hook(module, input, output):\n",
    "    hook_acts[\"activation\"] = output.detach().cpu()\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def get_module_by_name(model, path):\n",
    "    return reduce(getattr, path.split(\".\"), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5057f00d-a5df-4c3b-b308-fcf95605c40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794a8841-66c8-4048-967e-b734ee561091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50152249394748e89e7eb0f74c3ebf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference on cuda:0:   0%|                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4751a3db036a41d384f230bc2bc854bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference on cuda:1:   0%|                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations: torch.Size([400, 256, 2048])\n",
      "Sum of tokens recorded: 102400\n"
     ]
    }
   ],
   "source": [
    "from torch import amp  # for fix 3\n",
    "\n",
    "# === Config ===\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [example[\"image\"] for example in batch]\n",
    "    prompts = [example[\"prompt\"] for example in batch]\n",
    "    return {\"images\": images, \"prompts\": prompts}\n",
    "\n",
    "# === Split dataset across GPUs ===\n",
    "half = len(dataset) // 2\n",
    "datasets = [Subset(dataset, range(0, half)), Subset(dataset, range(half, len(dataset)))]\n",
    "devices = [\"cuda:0\", \"cuda:1\"]\n",
    "activation_lists = [[], []]\n",
    "token_counts = [[], []]\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm  # use auto for better notebook/thread compatibility\n",
    "from io import StringIO\n",
    "\n",
    "def run_inference(model, dataloader, device, activations_out, token_counts_out, layer_path):\n",
    "    model = model.to(device).eval()\n",
    "    hook_acts = {}\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        hook_acts[\"activation\"] = output.detach().cpu()\n",
    "\n",
    "    handle = get_module_by_name(model, layer_path).register_forward_hook(hook)\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        dataloader,\n",
    "        desc=f\"Inference on {device}\",\n",
    "        leave=True,\n",
    "        dynamic_ncols=True,\n",
    "        position=0 if device == \"cuda:0\" else 1  # prevent conflict\n",
    "    )\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        torch.cuda.empty_cache()\n",
    "        hook_acts.clear()\n",
    "\n",
    "        images = batch[\"images\"]\n",
    "        prompts = [f\"<image> {p}\" for p in batch[\"prompts\"]]\n",
    "\n",
    "        model_inputs = processor(\n",
    "            text=prompts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            do_rescale=False\n",
    "        )\n",
    "        model_inputs = {k: v.to(device, non_blocking=True) for k, v in model_inputs.items()}\n",
    "\n",
    "        with torch.no_grad(), amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            _ = model(**model_inputs)\n",
    "\n",
    "        act = hook_acts[\"activation\"]\n",
    "        if isinstance(act, tuple):\n",
    "            act = act[0]\n",
    "\n",
    "        activations_out.append(act.cpu().float())\n",
    "        token_counts_out.extend([act.shape[1]] * act.size(0))\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    \n",
    "from threading import Thread\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "dataloaders = [\n",
    "    DataLoader(datasets[0], batch_size=BATCH_SIZE, collate_fn=collate_fn,\n",
    "               num_workers=NUM_WORKERS, pin_memory=True),\n",
    "    DataLoader(datasets[1], batch_size=BATCH_SIZE, collate_fn=collate_fn,\n",
    "               num_workers=NUM_WORKERS, pin_memory=True)\n",
    "]\n",
    "\n",
    "models = [copy.deepcopy(model), copy.deepcopy(model)]\n",
    "threads = []\n",
    "\n",
    "for i in range(2):\n",
    "    t = Thread(target=run_inference, args=(\n",
    "        models[i], dataloaders[i], devices[i], activation_lists[i], token_counts[i], layer_name))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "all_activations = torch.cat(activation_lists[0] + activation_lists[1], dim=0)\n",
    "image_to_token_counts = token_counts[0] + token_counts[1]\n",
    "\n",
    "if all_activations.size(0) == 0:\n",
    "    raise ValueError(\"No valid activations collected!\")\n",
    "\n",
    "print(\"Collected activations:\", all_activations.shape)\n",
    "print(\"Sum of tokens recorded:\", sum(image_to_token_counts))\n",
    "\n",
    "del models, activation_lists, token_counts, datasets, dataloaders, hook_acts\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d97dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Convert to half precision before saving (cuts size in half)\n",
    "# # all_activations_fp16 = all_activations.half()\n",
    "# torch.save(all_activations.cpu(), \"all_activations_safaa.pt\")\n",
    "# torch.save(image_to_token_counts, \"image_to_token_counts_safaa.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5310fd-c2bb-46f9-84ad-7666caccc7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92486537-06d1-4e85-a136-02e0824bc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Load previously saved activations and token counts ===\n",
    "# all_activations = torch.load(\"all_activations_safaa.pt\")\n",
    "# image_to_token_counts = torch.load(\"image_to_token_counts_safaa.pt\")\n",
    "\n",
    "# print(\"Loaded activations shape:\", all_activations.shape)\n",
    "# print(\"Loaded token count length:\", len(image_to_token_counts))\n",
    "# all_activations = all_activations.to(device)  # or use your device variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd443669",
   "metadata": {},
   "source": [
    "### Train SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a45b6b0e-8735-46fd-9f1e-42033fd83412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhtsai\u001b[0m (\u001b[33mhtsai2025\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/henrytsai/safaa/roboterp/wandb/run-20250505_015745-72xfved9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/htsai2025/sparse-autoencoder/runs/72xfved9' target=\"_blank\">SAE_new_notebook_test</a></strong> to <a href='https://wandb.ai/htsai2025/sparse-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/htsai2025/sparse-autoencoder' target=\"_blank\">https://wandb.ai/htsai2025/sparse-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/htsai2025/sparse-autoencoder/runs/72xfved9' target=\"_blank\">https://wandb.ai/htsai2025/sparse-autoencoder/runs/72xfved9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|█████████████                                                                                                                                                            | 1/13 [00:00<00:08,  1.37it/s]/home/henrytsai/anaconda3/envs/safaa/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return F.linear(input, self.weight, self.bias)\n",
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss 0.083569\n",
      "Epoch 6 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss 0.039506\n",
      "Saved SAE to checkpoints/sae_2025-05-05_01-57-46.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run to sae_run_log.csv\n"
     ]
    }
   ],
   "source": [
    "# === Config and Hyperparameters ===\n",
    "HIDDEN_MULTIPLIER = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "SPARSITY_WEIGHT = 1e-2\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "NAME_OF_RUN = \"SAE_new_notebook_test\"\n",
    "csv_log_path = \"sae_run_log.csv\"\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Initialize Weights & Biases ===\n",
    "wandb.init(project=\"sparse-autoencoder\", name=NAME_OF_RUN, config={\n",
    "    \"hidden_multiplier\": HIDDEN_MULTIPLIER,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"sparsity_weight\": SPARSITY_WEIGHT,\n",
    "    \"n_epochs\": N_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "})\n",
    "\n",
    "# === Model Setup ===\n",
    "d_in = all_activations.shape[-1]\n",
    "d_hidden = HIDDEN_MULTIPLIER * d_in\n",
    "\n",
    "sae = SparseAutoencoder(d_in=d_in, d_hidden=d_hidden).to(device)\n",
    "sae = torch.nn.DataParallel(sae)  # Split across all visible GPUs\n",
    "\n",
    "optimizer = torch.optim.AdamW(sae.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# === DataLoader ===\n",
    "train_dataset = torch.utils.data.TensorDataset(all_activations)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Training Loop ===\n",
    "epoch_losses = []\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    sae.train()\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"Epoch {epoch+1} started...\")\n",
    "\n",
    "    for (batch,) in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        batch = batch.to(next(sae.parameters()).device, non_blocking=True)\n",
    "        recon, z = sae(batch)\n",
    "\n",
    "        loss = loss_fn(recon, batch) + SPARSITY_WEIGHT * torch.mean(torch.abs(z))\n",
    "        epoch_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataset)\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    wandb.log({\"loss\": avg_loss, \"epoch\": epoch + 1})\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == N_EPOCHS - 1:\n",
    "        print(f\"Epoch {epoch+1}: Loss {avg_loss:.6f}\")\n",
    "\n",
    "\n",
    "# === Save model checkpoint ===\n",
    "model_path = os.path.join(save_dir, f\"sae_{timestamp}.pth\")\n",
    "torch.save({\n",
    "    \"state_dict\": sae.state_dict(),\n",
    "    \"d_in\": d_in,\n",
    "    \"d_hidden\": d_hidden,\n",
    "}, model_path)\n",
    "wandb.save(model_path)\n",
    "print(f\"Saved SAE to {model_path}\")\n",
    "\n",
    "# === Plot and log loss curve ===\n",
    "plt.plot(range(1, N_EPOCHS + 1), epoch_losses, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"SAE Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "wandb.log({\"loss_curve\": wandb.Image(plt)})\n",
    "plt.show()\n",
    "\n",
    "# === CSV logging ===\n",
    "run_config = {\n",
    "    \"run_name\": NAME_OF_RUN,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"hidden_multiplier\": HIDDEN_MULTIPLIER,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"sparsity_weight\": SPARSITY_WEIGHT,\n",
    "    \"n_epochs\": N_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"d_in\": d_in,\n",
    "    \"d_hidden\": d_hidden,\n",
    "    \"final_loss\": avg_loss,\n",
    "    \"model_path\": model_path,\n",
    "}\n",
    "\n",
    "write_header = not os.path.exists(csv_log_path)\n",
    "with open(csv_log_path, mode=\"a\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=run_config.keys())\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(run_config)\n",
    "\n",
    "sae = sae.module\n",
    "\n",
    "print(f\"Logged run to {csv_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbbe0f",
   "metadata": {},
   "source": [
    "### Load previously trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f511334",
   "metadata": {},
   "source": [
    "### Visualize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc47e289-9f2d-4714-bf58-1886a6417ad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [16384] doesn't match the broadcast shape [256, 16384]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m batch \u001b[38;5;241m=\u001b[39m all_activations[i:i \u001b[38;5;241m+\u001b[39m batch_size]\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m sparse \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mactivation(sae\u001b[38;5;241m.\u001b[39mencoder(batch))  \u001b[38;5;66;03m# [B, d_hidden]\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m running_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)             \u001b[38;5;66;03m# [d_hidden]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m sparse_feature_list\u001b[38;5;241m.\u001b[39mappend(sparse\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [16384] doesn't match the broadcast shape [256, 16384]"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ==== CONFIG ====\n",
    "top_k = 5\n",
    "n_features_to_plot = 15\n",
    "batch_size = 4096\n",
    "num_workers = 8\n",
    "\n",
    "# ==== STREAM ENCODE ====\n",
    "sparse_feature_list = []\n",
    "running_sum = torch.zeros(sae.decoder.in_features, device=device)\n",
    "count = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(0, all_activations.size(0), batch_size):\n",
    "#         batch = all_activations[i:i + batch_size].to(device=device)\n",
    "#         sparse = sae.activation(sae.encoder(batch))\n",
    "#         running_sum += sparse.sum(dim=0)\n",
    "#         count += sparse.size(0)\n",
    "#         sparse_feature_list.append(sparse.cpu())\n",
    "\n",
    "sparse_feature_list = []\n",
    "running_sum = torch.zeros(sae.encoder.out_features, device=device)\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, all_activations.size(0), batch_size):\n",
    "        batch = all_activations[i:i + batch_size].to(device=device)\n",
    "        sparse = sae.activation(sae.encoder(batch))  # [B, d_hidden]\n",
    "        running_sum += sparse.sum(dim=0)             # [d_hidden]\n",
    "        count += sparse.size(0)\n",
    "        sparse_feature_list.append(sparse.cpu())\n",
    "\n",
    "sparse_features = torch.cat(sparse_feature_list, dim=0)\n",
    "\n",
    "# ==== TOKEN TO IMAGE INDEX MAPPING ====\n",
    "image_indices_per_token = []\n",
    "for img_idx, n_tokens in enumerate(image_to_token_counts):\n",
    "    image_indices_per_token.extend([img_idx] * n_tokens)\n",
    "image_indices_per_token = np.array(image_indices_per_token)\n",
    "\n",
    "# ==== TOP FEATURES ====\n",
    "mean_features = running_sum / count\n",
    "top_features = torch.topk(mean_features, k=n_features_to_plot).indices.tolist()\n",
    "print(f\"Top {n_features_to_plot} active features:\", top_features)\n",
    "\n",
    "# ==== IMAGE & TEXT LOADER ====\n",
    "def load_img_and_text(idx):\n",
    "    img_idx = int(image_indices_per_token[idx])\n",
    "    entry = dataset[img_idx]\n",
    "    # Convert image tensor to PIL and then to numpy for plotting\n",
    "    image_tensor = entry[\"image\"]\n",
    "    image_pil = to_pil_image(image_tensor)\n",
    "    prompt = entry[\"prompt\"]\n",
    "    return np.array(image_pil), prompt\n",
    "\n",
    "# ==== DISPLAY LOOP ====\n",
    "for feature_idx in top_features:\n",
    "    activations = sparse_features[:, feature_idx]\n",
    "    topk_indices = torch.topk(activations, k=top_k).indices.cpu().tolist()\n",
    "\n",
    "    # Load image-prompt pairs\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        img_and_text = list(executor.map(load_img_and_text, topk_indices))\n",
    "\n",
    "    images, prompts = zip(*img_and_text)\n",
    "\n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, top_k, figsize=(3 * top_k, 4))\n",
    "    fig.suptitle(f\"Feature {feature_idx} (Top {top_k} Activations)\", fontsize=14)\n",
    "\n",
    "    for ax, img, text in zip(axes, images, prompts):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(text, fontsize=8, wrap=True)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906de685",
   "metadata": {},
   "source": [
    "### Sparsity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107237bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `sae` is already loaded and on the correct device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sae = sae.to(device)\n",
    "sae.eval()\n",
    "\n",
    "# Compute feature usage (mean |z|) without storing all z\n",
    "d_hidden = sae.module.encoder.out_features if isinstance(sae, torch.nn.DataParallel) else sae.encoder.out_features\n",
    "running_sum = torch.zeros(d_hidden, device=device)\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (batch,) in train_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        _, z = sae(batch)                      # shape: [B, d_hidden] or [B, T, d_hidden]\n",
    "        z = z.view(-1, z.shape[-1])            # flatten in case it's [B, T, d_hidden]\n",
    "        running_sum += torch.sum(torch.abs(z), dim=0)\n",
    "        count += z.shape[0]\n",
    "\n",
    "feature_usage = running_sum / count\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(feature_usage.cpu().numpy(), bins=100)\n",
    "plt.title(\"Feature Usage Distribution\")\n",
    "plt.xlabel(\"Mean |z|\")\n",
    "plt.ylabel(\"# Features\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0c9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (safaa)",
   "language": "python",
   "name": "safaa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
