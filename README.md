# VLM Comparison Tools

This repository contains two Streamlit applications for comparing outputs from vision-language models (VLMs). These tools are designed to conduct blind, side-by-side evaluations of either images or captions generated by base and fine-tuned models.

---

## ğŸ–¼ï¸ 1. Image Comparison Tool (`sae_comparison_streamlit.py`)

This tool presents two randomly selected images â€” one from a base model and one from a fine-tuned model â€” and asks users to vote for the better one. It randomizes the display order and logs user preferences.

### ğŸ”§ Key Features

- Randomized blind comparison of images  
- Vote logging with timestamps and user identification  
- Displays total number of votes submitted  

### ğŸ› ï¸ Configuration

Update the paths in the script as needed:

BASE_FOLDER = "/path/to/base/images"
FINETUNED_FOLDER = "/path/to/finetuned/images"
VOTE_LOG = "/path/to/vote_log.csv"

streamlit run sae_comparison_streamlit.py

## 2. Caption Comparison Tool (finetuning_comparison_streamlit.py)
This tool presents a single image along with two captions (from base and fine-tuned models) and asks the user to vote on which caption better describes the image. Captions are matched to the same image ID and randomized left/right.

### ğŸ”§ Key Features
Side-by-side blind caption comparison

User voting with session state

Support for both .jpg and .txt file pairs

Vote log includes both captions and source labels

###ğŸ› ï¸ Configuration
Update the script paths:
BASE_FOLDER = "/path/to/base/captions_and_images"
FINETUNED_FOLDER = "/path/to/finetuned/captions_and_images"
VOTE_LOG = "/path/to/vote_log.csv"

streamlit run finetuning_comparison_streamlit.py
