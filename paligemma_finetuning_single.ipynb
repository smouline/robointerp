{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3MMAcssHTML"
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"/site-assets/css/style.css\">\n",
    "<link rel=\"stylesheet\" href=\"/site-assets/css/gemma.css\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiJG9Do4_-sm"
   },
   "source": [
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "zGLIp1Cx3_CX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "KAGGLE_USERNAME = \"\" #FILL IN\n",
    "KAGGLE_KEY = \"\" #FILL IN\n",
    "os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
    "os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4, 5, 6, 7\" #FILL IN\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "DfxKb3F839Ks"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import flax.serialization\n",
    "import msgpack\n",
    "\n",
    "if not os.path.exists(\"big_vision_repo\"):\n",
    "  !git clone --quiet --branch=main --depth=1 \\\n",
    "     https://github.com/google-research/big_vision big_vision_repo\n",
    "\n",
    "# Append big_vision code to python import path\n",
    "if \"big_vision_repo\" not in sys.path:\n",
    "  sys.path.append(\"big_vision_repo\")\n",
    "\n",
    "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
    "!pip3 install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTfe2k8J4Bw0"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import functools\n",
    "import html\n",
    "import io\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "from big_vision.trainers.proj.paligemma import predict_fns\n",
    "\n",
    "import big_vision.datasets.jsonl\n",
    "import big_vision.utils\n",
    "import big_vision.sharding\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "\n",
    "\n",
    "backend = jax.extend.backend.get_backend()\n",
    "print(f\"JAX version:  {jax.__version__}\")\n",
    "print(f\"JAX platform: {backend.platform}\")\n",
    "print(f\"JAX devices:  {jax.device_count()}\")\n",
    "\n",
    "def save_checkpoint(params, step, save_dir=\"checkpoints\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    path = os.path.join(save_dir, f\"checkpoint_paligemma_finetuning_896_correct_describe_prompt{step:04d}.msgpack\")\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(flax.serialization.to_bytes(params))\n",
    "    print(f\"âœ… Saved checkpoint to: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQNOTfF24AV4"
   },
   "outputs": [],
   "source": [
    "#Download the model checkpoint if not already available\n",
    "\n",
    "# Use these for Paligemma 2 with 896x896 images\n",
    "LLM_VARIANT = \"gemma2_2b\"\n",
    "MODEL_PATH = \"./paligemma2-3b-pt-896.b16.npz\"\n",
    "KAGGLE_HANDLE = \"google/paligemma-2/jax/paligemma2-3b-pt-896\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "  print(\"Downloading the checkpoint from Kaggle, this could take a few minutes....\")\n",
    "  MODEL_PATH = kagglehub.model_download(KAGGLE_HANDLE, MODEL_PATH)\n",
    "  print(f\"Model path: {MODEL_PATH}\")\n",
    "\n",
    "TOKENIZER_PATH = \"./paligemma_tokenizer.model\"\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "  print(\"Downloading the model tokenizer...\")\n",
    "  !gsutil cp gs://big_vision/paligemma_tokenizer.model {TOKENIZER_PATH}\n",
    "  print(f\"Tokenizer path: {TOKENIZER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "1aghcULcEdtv"
   },
   "outputs": [],
   "source": [
    "# Load the Baseline Paligemma Model\n",
    "\n",
    "model_config = ml_collections.FrozenConfigDict({\n",
    "    \"llm\": {\"vocab_size\": 257216, \"variant\": LLM_VARIANT, \"final_logits_softcap\": 0.0},\n",
    "    \"img\": {\"variant\": \"So400m/14\", \"pool_type\": \"none\", \"scan\": True, \"dtype_mm\": \"float16\"}\n",
    "})\n",
    "model = paligemma.Model(**model_config)\n",
    "tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)\n",
    "\n",
    "params = paligemma.load(None, MODEL_PATH, model_config)\n",
    "\n",
    "decode_fn = predict_fns.get_all(model)['decode']\n",
    "decode = functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWOdf_fw2SAO"
   },
   "outputs": [],
   "source": [
    "def is_trainable_param(name, param):\n",
    "  if name.startswith(\"llm/layers/attn/\"):\n",
    "    return True\n",
    "  if name.startswith(\"llm/\"):\n",
    "    return False\n",
    "  if name.startswith(\"img/\"):\n",
    "    return False\n",
    "  raise ValueError(f\"Unexpected param name {name}\")\n",
    "\n",
    "trainable_mask = big_vision.utils.tree_map_with_names(is_trainable_param, params)\n",
    "\n",
    "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))\n",
    "print(\"Using mesh with devices:\", mesh.devices)\n",
    "print(\"jax.devices():\", jax.devices())\n",
    "data_sharding = jax.sharding.NamedSharding(\n",
    "    mesh, jax.sharding.PartitionSpec(\"data\"))\n",
    "\n",
    "params_sharding = big_vision.sharding.infer_sharding(\n",
    "    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Some donated buffers were not usable\")\n",
    "\n",
    "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
    "def maybe_cast_to_f32(params, trainable):\n",
    "  return jax.tree.map(lambda p, m: p.astype(jnp.float32)\n",
    "                      if m else p.astype(jnp.float16),\n",
    "                      params, trainable)\n",
    "\n",
    "params, treedef = jax.tree.flatten(params)\n",
    "sharding_leaves = jax.tree.leaves(params_sharding)\n",
    "trainable_leaves = jax.tree.leaves(trainable_mask)\n",
    "for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):\n",
    "  params[idx] = big_vision.utils.reshard(params[idx], sharding)\n",
    "  jax.debug.print(\"Casting param {} with shape {}\", idx, params[idx].shape)\n",
    "  params[idx] = maybe_cast_to_f32(params[idx], trainable)\n",
    "  params[idx].block_until_ready()\n",
    "params = jax.tree.unflatten(treedef, params)\n",
    "\n",
    "# Print params to show what the model is made of.\n",
    "def parameter_overview(params):\n",
    "  for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n",
    "    print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n",
    "\n",
    "print(\" == Model params == \")\n",
    "parameter_overview(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 32\n",
    "NUM_FRAMES = 20\n",
    "\n",
    "train_ds = tfds.load(\"droid_100\", split=\"train\", data_dir=\"gs://gresearch/robotics\")\n",
    "val_ds = tfds.load(\"droid_100\", split=\"train\", data_dir=\"gs://gresearch/robotics\")\n",
    "\n",
    "def subsample_frames(images, num_samples):\n",
    "    if len(images) <= num_samples:\n",
    "        return images \n",
    "    indices = np.linspace(0, len(images) - 1, num=num_samples, dtype=int)\n",
    "    return [images[i] for i in indices]\n",
    "\n",
    "def stack_images_horizontally(images):\n",
    "    widths, heights = zip(*(img.size for img in images))\n",
    "    new_im = Image.new('RGB', (sum(widths), max(heights)))\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        new_im.paste(im, (x_offset, 0))\n",
    "        x_offset += im.width\n",
    "    return new_im\n",
    "\n",
    "def stack_images_grid(images, frames_per_row=4):\n",
    "    rows = [images[i:i+frames_per_row] for i in range(0, len(images), frames_per_row)]\n",
    "    stacked_rows = [stack_images_horizontally(row) for row in rows]\n",
    "    return np.vstack([np.asarray(r) for r in stacked_rows])\n",
    "\n",
    "def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
    "    separator = \"\\n\"\n",
    "    tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
    "    mask_ar = [0] * len(tokens)\n",
    "    mask_loss = [0] * len(tokens)\n",
    "\n",
    "    if suffix:\n",
    "        suffix = tokenizer.encode(suffix, add_eos=True)\n",
    "        tokens += suffix\n",
    "        mask_ar += [1] * len(suffix)\n",
    "        mask_loss += [1] * len(suffix)\n",
    "\n",
    "    mask_input = [1] * len(tokens)\n",
    "\n",
    "    if seqlen:\n",
    "        padding = [0] * max(0, seqlen - len(tokens))\n",
    "        tokens = tokens[:seqlen] + padding\n",
    "        mask_ar = mask_ar[:seqlen] + padding\n",
    "        mask_loss = mask_loss[:seqlen] + padding\n",
    "        mask_input = mask_input[:seqlen] + padding\n",
    "\n",
    "    return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))\n",
    "\n",
    "def postprocess_tokens(tokens):\n",
    "    tokens = tokens.tolist()\n",
    "    try:\n",
    "        eos_pos = tokens.index(tokenizer.eos_id())\n",
    "        tokens = tokens[:eos_pos]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "def openx_iterator(ds, repeat=False, shuffle=False, train=True):\n",
    "    ds = ds.shuffle(1000) if shuffle else ds\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "\n",
    "    for episode in tfds.as_numpy(ds):\n",
    "        try:\n",
    "            steps = list(episode[\"steps\"])\n",
    "            if len(steps) < NUM_FRAMES:\n",
    "                continue\n",
    "            print(steps[0][\"language_instruction\"])\n",
    "            if (len(steps[0][\"language_instruction\"].decode(\"utf-8\").lower()) <= 0 and train) or (not train and len(steps[0][\"language_instruction\"].decode(\"utf-8\").lower()) > 0):\n",
    "                continue\n",
    "            all_images = [Image.fromarray(step[\"observation\"][\"exterior_image_1_left\"])\n",
    "                          for step in steps]\n",
    "            num_frames = 20 if train else 4\n",
    "            subsampled = subsample_frames(all_images, num_frames)\n",
    "            for images in subsampled:\n",
    "                images = [images]\n",
    "                display_image = stack_images_grid(images)\n",
    "                display_image = np.asarray(display_image).astype(np.uint8)\n",
    "\n",
    "                model_image = Image.fromarray(display_image).resize((896, 896), Image.BILINEAR)\n",
    "                model_image = np.asarray(model_image).astype(np.float32) / 127.5 - 1.0  # Normalize to [-1, 1]\n",
    "                model_image = jax.device_put(model_image, device=jax.devices(\"cpu\")[0])\n",
    "\n",
    "                instruction = steps[0][\"language_instruction\"].decode(\"utf-8\").lower()\n",
    "                prefix = \"describe the task the robot is taking:\"\n",
    "                tokens, mask_ar, mask_loss, mask_input = preprocess_tokens(prefix, instruction, SEQLEN)\n",
    "\n",
    "                yield {\n",
    "                    \"image\": model_image,        \n",
    "                    \"image_raw\": display_image, \n",
    "                    \"text\": np.asarray(tokens),\n",
    "                    \"mask_ar\": np.asarray(mask_ar),\n",
    "                    \"mask_loss\": np.asarray(mask_loss),\n",
    "                    \"mask_input\": np.asarray(mask_input),\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "def train_data_iterator():\n",
    "    return openx_iterator(train_ds, repeat=True, shuffle=True, train=True)\n",
    "\n",
    "def validation_data_iterator():\n",
    "    return openx_iterator(val_ds, repeat=False, shuffle=False, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzJfb5t0nsLq"
   },
   "outputs": [],
   "source": [
    "# View Training Examples\n",
    "\n",
    "def render_inline(image):\n",
    "    image = jax.device_get(image)\n",
    "    image = np.array(image)\n",
    "\n",
    "    if image.dtype == np.float32:\n",
    "        image = ((image + 1.0) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    image = image.squeeze()  # Remove singleton dims like (1, 1, 3)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    with io.BytesIO() as buffer:\n",
    "        image.save(buffer, format='jpeg')\n",
    "        encoded = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "    return f'data:image/jpeg;base64,{encoded}'\n",
    "\n",
    "def render_example(image, caption):\n",
    "    return f\"\"\"\n",
    "        <div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
    "            <img style=\"max-width:100%;\" src=\"{render_inline(image)}\" />\n",
    "            <p style=\"width:256px; margin:10px; font-size:small;\">{html.escape(caption)}</p>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_out = \"\"\n",
    "for idx, example in zip(range(20), train_data_iterator()):\n",
    "    caption = postprocess_tokens(example[\"text\"])  # detokenize model input\n",
    "    caption = caption[len(\"describe the task the robot is taking:\\n\"):]        # remove prompt prefix\n",
    "    html_out += render_example(example[\"image_raw\"], caption)\n",
    "\n",
    "display(HTML(html_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "dwUV_imW3WQJ"
   },
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, donate_argnums=(0,))\n",
    "def update_fn(params, batch, learning_rate):\n",
    "  imgs, txts, mask_ar = batch[\"image\"], batch[\"text\"], batch[\"mask_ar\"]\n",
    "\n",
    "  def loss_fn(params):\n",
    "    text_logits, _ = model.apply({\"params\": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True)\n",
    "    logp = jax.nn.log_softmax(text_logits, axis=-1)\n",
    "    mask_loss = batch[\"mask_loss\"][:, 1:]\n",
    "    targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])\n",
    "    token_pplx = jnp.sum(logp * targets, axis=-1)\n",
    "    example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1) \n",
    "    example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)\n",
    "\n",
    "    return jnp.mean(example_loss)\n",
    "\n",
    "  loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "  def apply_grad(param, gradient, trainable):\n",
    "    if not trainable: return param\n",
    "    return param - learning_rate * gradient\n",
    "\n",
    "  params = jax.tree_util.tree_map(apply_grad, params, grads, trainable_mask)\n",
    "\n",
    "  return params, loss\n",
    "\n",
    "def make_predictions(data_iterator, *, num_examples=None,\n",
    "                     batch_size=4, seqlen=SEQLEN, sampler=\"greedy\"):\n",
    "  outputs = []\n",
    "  while True:\n",
    "    examples = []\n",
    "    try:\n",
    "      for _ in range(batch_size):\n",
    "        examples.append(next(data_iterator))\n",
    "        examples[-1][\"_mask\"] = np.array(True)\n",
    "    except StopIteration:\n",
    "      if len(examples) == 0:\n",
    "        return outputs\n",
    "\n",
    "    while len(examples) % batch_size:\n",
    "      examples.append(dict(examples[-1]))\n",
    "      examples[-1][\"_mask\"] = np.array(False)\n",
    "\n",
    "    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
    "    batch = big_vision.utils.reshard(batch, data_sharding)\n",
    "\n",
    "    tokens = decode({\"params\": params}, batch=batch,\n",
    "                    max_decode_len=seqlen, sampler=sampler)\n",
    "\n",
    "    tokens, mask = jax.device_get((tokens, batch[\"_mask\"]))\n",
    "    tokens = tokens[mask]\n",
    "    responses = [postprocess_tokens(t) for t in tokens]\n",
    "\n",
    "    for example, response in zip(examples, responses):\n",
    "      outputs.append((example[\"image\"], response))\n",
    "      if num_examples and len(outputs) >= num_examples:\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "067wj_6bZAG3"
   },
   "outputs": [],
   "source": [
    "# Finetune the Model\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "TRAIN_EXAMPLES = 1000\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE\n",
    "EVAL_STEPS = 100\n",
    "\n",
    "train_data_it = train_data_iterator()\n",
    "\n",
    "sched_fn = big_vision.utils.create_learning_rate_schedule(\n",
    "    total_steps=TRAIN_STEPS+1, base=LEARNING_RATE,\n",
    "    decay_type=\"cosine\", warmup_percent=0.10)\n",
    "for step in range(1, TRAIN_STEPS+1):\n",
    "  # Make list of N training examples.\n",
    "  examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n",
    "  print(examples[0]['text'])\n",
    "\n",
    "  batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
    "  batch = big_vision.utils.reshard(batch, data_sharding)\n",
    "\n",
    "  learning_rate = sched_fn(step)\n",
    "  params, loss = update_fn(params, batch, learning_rate)\n",
    "\n",
    "  loss = jax.device_get(loss)\n",
    "  print(f\"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}\")\n",
    "\n",
    "  if (step % EVAL_STEPS) == 0:\n",
    "    print(f\"Model predictions at step {step}\")\n",
    "    html_out = \"\"\n",
    "    for image, caption in make_predictions(\n",
    "        validation_data_iterator(), num_examples=4, batch_size=4):\n",
    "      html_out += render_example(image, caption)\n",
    "      print(\"Caption\", caption)\n",
    "    display(HTML(html_out))\n",
    "    save_checkpoint(params, step)\n",
    "\n",
    "save_checkpoint(params, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgUhEKjzPdMQ"
   },
   "outputs": [],
   "source": [
    "# Perform validation\n",
    "\n",
    "html_out = \"\"\n",
    "for image, caption in make_predictions(validation_data_iterator(), batch_size=4):\n",
    "  html_out += render_example(image, caption)\n",
    "display(HTML(html_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Checkpoint\n",
    "\n",
    "LLM_VARIANT = \"gemma2_2b\"\n",
    "\n",
    "model_config = ml_collections.FrozenConfigDict({\n",
    "    \"llm\": {\"vocab_size\": 257_152, \"variant\": LLM_VARIANT, \"final_logits_softcap\": 0.0},\n",
    "    \"img\": {\"variant\": \"So400m/14\", \"pool_type\": \"none\", \"scan\": True, \"dtype_mm\": \"float16\"}\n",
    "})\n",
    "init_params = paligemma.load(None, MODEL_PATH, model_config)\n",
    "\n",
    "\n",
    "def load_checkpoint(template_params, step, save_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    template_params: a pytree of the same structure as your saved params,\n",
    "                     e.g. the output of your model's init().\n",
    "    step: integer step number that matches the filename.\n",
    "    \"\"\"\n",
    "    path = \"/home/henrytsai/checkpoints/checkpoint_0001.msgpack\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "    return flax.serialization.from_bytes(template_params, raw)\n",
    "\n",
    "params = load_checkpoint(init_params, step=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "fine-tuning-paligemma.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "dhruv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
